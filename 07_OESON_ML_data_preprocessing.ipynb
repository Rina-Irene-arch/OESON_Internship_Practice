{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VUC7OXNrjJp3TEcR50F7RO6pk6WkzASO","timestamp":1719400154201}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WOw8yMd1VlnD"},"source":["# Machine Learning - Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"NvUGC8QQV6bV"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"wfFEXZC0WS-V"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhYaZ-ENV_c5"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"aqHTg9bxWT_u"},"source":["# preparation phase of a machine learning process, specifically for supervised learning\n","# 1. Reading the Dataset:\n","dataset = pd.read_csv('Data.csv')\n","\n","#2. Separating Features (Predictors - independent variables):\n","'''\n","This line extracts all columns except the last one as features (predictors).\n","The .iloc[:, :-1] selects all rows (:) and all columns except the last one (:-1).\n","The .values at the end converts the DataFrame slice into a NumPy array,\n","which is often used for feeding data into machine learning models.\n","'''\n","X = dataset.iloc[:, :-1].values #feature\n","\n","#3. Separating Target Variable:\n","'''\n","This line extracts the last column of the DataFrame as the target variable (y).\n","The .iloc[:, -1] specifically selects all rows and only the last column,\n","which is typically the variable you want to predict.\n","Again, .values converts it into a NumPy array.\n","'''\n","\n","y = dataset.iloc[:, -1].values #target"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HOA5R3xpDjE","executionInfo":{"status":"ok","timestamp":1719477596417,"user_tz":-180,"elapsed":272,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"4b0c07c7-4701-493f-8489-2a302f88396d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYZWdlvDpIXN","executionInfo":{"status":"ok","timestamp":1719477602041,"user_tz":-180,"elapsed":439,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"8e4294d6-ca4a-4914-ceb6-151aaf860a5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}]},{"cell_type":"markdown","source":["## Taking care of missing data"],"metadata":{"id":"TMFJeayRpXwW"}},{"cell_type":"code","source":["#1. Importing the SimpleImputer Class:\n","'''\n","This imports the SimpleImputer class, which provides basic strategies for imputing missing values,\n","including using the mean, median, mode, or a constant value.\n","'''\n","from sklearn.impute import SimpleImputer # Sci Kit learn\n","#2. Creating an Imputer Instance:\n","'''\n","missing_values=np.nan: This parameter specifies what the imputer should treat as a missing value.\n","Here, np.nan is used to indicate that any NaN value in the data should be considered missing.\n","strategy='mean': This sets the imputation strategy.\n","'mean' means that the imputer will replace missing values using the mean value of each column\n","where the missing values are located.\n","'''\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","#3. Fitting the Imputer:\n","'''\n","This line applies the fit method of the imputer to a slice of the data (X[:, 1:3]),\n","which includes only the second and third columns of X (Python uses 0-based indexing).\n","The fit method calculates the necessary statistics (in this case, the mean) for each column\n","that it will later use to perform the actual imputation.\n","This calculation is based only on the columns specified.\n","'''\n","imputer.fit(X[:, 1:3])\n","#4. Transforming the Data:\n","'''\n","After fitting, this line uses the transform method to replace the missing values in the original data slice (X[:, 1:3])\n","with the means computed by the fit method.\n","The result is that the original missing values in these columns are now filled with their respective mean values.\n","'''\n","X[:, 1:3] = imputer.transform(X[:, 1:3])"],"metadata":{"id":"llob_UBfpW1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfK7V3Vcv-21","executionInfo":{"status":"ok","timestamp":1719477612272,"user_tz":-180,"elapsed":260,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"6dd865e9-37ee-4c0d-af5d-4ee4f9be1a1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","source":["## Encoding categorical data\n","performing on categorical values only"],"metadata":{"id":"aAGPqmkYwXx4"}},{"cell_type":"markdown","source":["### - Encoding the Independent Variable"],"metadata":{"id":"LxX7x4m6wnk4"}},{"cell_type":"code","source":["'''\n","The code you've provided deals with categorical data transformation using the ColumnTransformer\n","and OneHotEncoder from Scikit-learn's preprocessing\n","'''\n","# 1. Importing Necessary Classes:\n","'''\n","-ColumnTransformer: This class allows different columns of the input data\n","to be transformed in different ways, which is essential\n","when you have both numerical and categorical data.\n","'''\n","from sklearn.compose import ColumnTransformer\n","'''\n","-OneHotEncoder: This class transforms categorical variables into a form that could be provided\n","to ML algorithms to do a better job in prediction.\n","'''\n","from sklearn.preprocessing import OneHotEncoder\n","\n","#2. Setting Up the Column Transformer:\n","'''\n","- transformers: This parameter specifies the transformations to apply.\n","Each transformer is a tuple containing:\n","A name ('encoder' in this case, but it's arbitrary and can be anything descriptive).\n","The transformation to apply (OneHotEncoder() here, which encodes categorical features as a one-hot numeric array).\n","The column indices to apply this transformation to ([0] indicating the first column).\n","- remainder='passthrough': This tells the ColumnTransformer to pass through the other columns\n","of the dataset without transforming them. Only the specified columns ([0] in this case) will be transformed.\n","\n","'''\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n","\n","#3. Applying the Transformation:\n","'''\n","- fit_transform(): This method first fits the transformer (calculates the necessary parameters for transformation) and then transforms the data.\n","It's applied to the entire X dataset but only transforms the columns specified.\n","- np.array(...): The result of fit_transform() is converted into a NumPy array. This is common practice if you want to ensure the output is suitable for use with other Scikit-learn utilities or ML algorithms, as they typically operate on NumPy arrays.\n","'''\n","X = np.array(ct.fit_transform(X))"],"metadata":{"id":"2zwMCNdFxAxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7lYHZWEzpS7","executionInfo":{"status":"ok","timestamp":1719477627174,"user_tz":-180,"elapsed":430,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"8ff9700f-76f2-4d31-c426-49d0cf7ef788"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","source":["### Encoding the Dependent Variable"],"metadata":{"id":"0CfsZNJrzxtU"}},{"cell_type":"code","source":["'''\n","Uses the LabelEncoder class from the Scikit-learn library to encode target labels with value between 0 and n_classes-1.\n","This is often used in machine learning when the target variable y is categorical (non-numeric).\n","'''\n","#1. Importing the LabelEncoder Class:\n","'''\n","This imports the LabelEncoder class, which is designed to normalize labels\n","such that they contain only values between 0 and the number of classes minus one.\n","'''\n","from sklearn.preprocessing import LabelEncoder\n","#2. Creating an Instance of LabelEncoder:\n","'''\n","This line creates an instance of LabelEncoder named le.\n","This instance will be used to fit to the data and then transform it.\n","'''\n","le = LabelEncoder()\n","#3. Fitting the Encoder and Transforming the Target Variable:\n","'''\n","- fit_transform(): This method first fits the label encoder to y (learning the labels that exist within y)\n"," and then transforms y into an array of integers.\n"," Each unique label in y is assigned a unique integer based on alphabetical order.\n","'''\n","y = le.fit_transform(y)\n","\n","'''\n","Outcome: After this transformation, your target variable y will consist solely of integer labels.\n","This is particularly necessary for machine learning models in Scikit-learn that require the target input\n","to be numeric. It’s commonly used in classification tasks where the target labels are nominal\n","(e.g., types of species, categories of products). This approach simplifies handling categorical labels\n","and is a prerequisite for most Scikit-learn classifiers that do not natively handle categorical data.\n","'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"RU4You49z0Uy","executionInfo":{"status":"ok","timestamp":1719477635249,"user_tz":-180,"elapsed":446,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"8cec684d-de13-42a4-d08a-2f37ce456174"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nOutcome: After this transformation, your target variable y will consist solely of integer labels. \\nThis is particularly necessary for machine learning models in Scikit-learn that require the target input \\nto be numeric. It’s commonly used in classification tasks where the target labels are nominal \\n(e.g., types of species, categories of products). This approach simplifies handling categorical labels \\nand is a prerequisite for most Scikit-learn classifiers that do not natively handle categorical data.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08TFmAaT1fUI","executionInfo":{"status":"ok","timestamp":1719477650168,"user_tz":-180,"elapsed":432,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"4101647d-7375-4bf6-f110-4e236a062727"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1 0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"3abSxRqvWEIB"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","metadata":{"id":"hm48sif-WWsh"},"source":["# 1. Importing the Function:\n","'''\n","This line imports the train_test_split function from the model_selection module\n","of the sklearn (Scikit-learn) library.\n","This function is specifically designed to randomly partition the data into training and testing sets.\n","'''\n","from sklearn.model_selection import train_test_split\n","#2. Splitting the Data:\n","'''\n","test_size=0.2: This argument specifies that 20% of the data will be set aside as the test dataset.\n","Accordingly, the remaining 80% will be used for training the model.\n","random_state=0: This is a seed value for random number generation, ensuring the split is reproducible.\n","It means that every time you run this code with the same random_state,\n","you'll get the same split, which is helpful for debugging and comparing model performance across different runs.\n","'''\n","'''\n","Result: The function returns four subsets:\n","\n","X_train: The features for training the model.\n","X_test: The features for testing the model.\n","y_train: The target variable for training.\n","y_test: The target variable for testing.\n","These subsets are used to train a model on X_train and y_train,\n","and then to test it on X_test and y_test to evaluate its performance.\n","This practice helps in understanding how well the model is likely to perform on unseen data,\n","thereby providing an estimation of its generalization ability.\n","'''\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frQ0OQjB15aM","executionInfo":{"status":"ok","timestamp":1719477711905,"user_tz":-180,"elapsed":427,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"19221f84-b8d0-4605-baf9-486c3216f799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 37.0 67000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"]}]},{"cell_type":"code","source":["print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx5Rsb7015Fh","executionInfo":{"status":"ok","timestamp":1719477759123,"user_tz":-180,"elapsed":422,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"c5272864-b4e2-4c68-feef-ad8d98eed0e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]]\n"]}]},{"cell_type":"code","source":["print(y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr_uHaVg2Db3","executionInfo":{"status":"ok","timestamp":1719477761174,"user_tz":-180,"elapsed":468,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"8fc901e9-83a3-4663-f9bd-6a7c45a86002"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1 0 1 0 0 1]\n"]}]},{"cell_type":"code","source":["print(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZCP6j_U2E4P","executionInfo":{"status":"ok","timestamp":1719477766615,"user_tz":-180,"elapsed":447,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"243dc864-b87a-42ed-ff2f-5acd71d68e3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0]\n"]}]},{"cell_type":"markdown","source":["## Feature Scaling"],"metadata":{"id":"kpSccfsm3QJ1"}},{"cell_type":"code","source":["'''\n","The code you've shared is used to standardize (scale) features of a dataset using the StandardScaler from Scikit-learn.\n","This preprocessing step is crucial for many machine learning algorithms that are sensitive to the scale of input features,\n","such as support vector machines and k-nearest neighbors.\n","'''\n","#1.Importing the StandardScaler Class:\n","'''\n","This imports the StandardScaler class, which standardizes features by removing\n","the mean and scaling to unit variance.\n","This is done feature-wise (independently for each feature)\n","by computing the relevant statistics on the samples in the training set.\n","'''\n","from sklearn.preprocessing import StandardScaler\n","\n","#2. Creating an Instance of StandardScaler:\n","'''\n","This line creates an instance of StandardScaler called sc.\n","This object will be used to compute the mean and standard deviation on a set of data,\n","which can then be used to scale the data.\n","'''\n","sc = StandardScaler()\n","\n","#3. Fitting the Scaler to the Training Data and Transforming:\n","'''\n","-fit_transform(): This method fits the scaler to the data by calculating the mean\n","and standard deviation of each feature, and then transforms the data by standardizing it\n","using these calculated values.\n","This method is applied to the columns from the third to the last in X_train.\n","The slicing [:, 3:] indicates that only the features from the third column onward are being scaled.\n","The transformed data (standardized) replaces the original data in these columns.\n","'''\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n","\n","#4. Transforming the Test Data:\n","'''\n","- transform(): This method uses the same mean and standard deviation,\n","which were computed from the training data, to standardize the test data.\n","It’s crucial to use the same scaling parameters on the test data\n","to ensure consistency between the training and testing phases.\n","This helps in avoiding any bias that might occur due to different scaling.\n","As with the training set, this scaling affects only the columns from the third to the last in X_test.\n","'''\n","\n","X_test[:, 3:] = sc.transform(X_test[:, 3:])\n","\n","'''\n","Outcome: Both the training and test datasets are scaled so that each feature (from the third column onward)\n","now has zero mean and unit variance.\n","This makes the algorithm less likely to be skewed by features with larger ranges\n","and improves the performance and stability of many machine learning algorithms.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"L3xPrzAD3Wo-","executionInfo":{"status":"ok","timestamp":1719478305124,"user_tz":-180,"elapsed":300,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"0d01bc88-ad4e-4c96-e807-e7d91b3abe63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nOutcome: Both the training and test datasets are scaled so that each feature (from the third column onward) \\nnow has zero mean and unit variance. \\nThis makes the algorithm less likely to be skewed by features with larger ranges \\nand improves the performance and stability of many machine learning algorithms.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZZrximZ5U3W","executionInfo":{"status":"ok","timestamp":1719478345662,"user_tz":-180,"elapsed":278,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"a805a0a1-c39b-4bc5-dcff-0e5c418426b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 0.2630675731713538 0.1238147854838185]\n"," [1.0 0.0 0.0 -0.25350147960148617 0.4617563176278856]\n"," [0.0 0.0 1.0 -1.9753983221776195 -1.5309334063940294]\n"," [0.0 0.0 1.0 0.05261351463427101 -1.1114197802841526]\n"," [1.0 0.0 0.0 1.6405850472322605 1.7202971959575162]\n"," [0.0 0.0 1.0 -0.08131179534387283 -0.16751412153692966]\n"," [1.0 0.0 0.0 0.9518263102018072 0.9861483502652316]\n"," [1.0 0.0 0.0 -0.5978808481167128 -0.48214934111933727]]\n"]}]},{"cell_type":"code","source":["print(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qILH6ea5ZHv","executionInfo":{"status":"ok","timestamp":1719478364056,"user_tz":-180,"elapsed":310,"user":{"displayName":"Rina Irene Rafalski","userId":"17396273602632308485"}},"outputId":"c9a66345-4bef-4c2f-c020-ad60d570d402"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 -1.4588292694047795 -0.9016629672292141]\n"," [0.0 1.0 0.0 1.984964415747487 2.139810822067393]]\n"]}]}]}
